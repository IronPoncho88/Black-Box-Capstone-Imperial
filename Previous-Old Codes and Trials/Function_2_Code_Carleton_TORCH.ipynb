{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch.quasirandom import SobolEngine\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from dataclasses import dataclass\n",
    "import gpytorch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.acquisition import UpperConfidenceBound, ExpectedImprovement\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a dataclass for our state\n",
    "@dataclass\n",
    "class TurboState:\n",
    "    dim: int # dimension of the problem, aka input dimension\n",
    "    batch_size: int = 1 # we could do batch optimization, but the capstone only does one query at a time\n",
    "    length: float = 0.3 # the length of the current trust region\n",
    "    length_min: float = 0.01 # minimum length for the trust region\n",
    "    length_max: float = 1.0 # maximum length for the trust region\n",
    "    failure_counter: int = 0 # initialize counter of the number of failures to improve on the best observation\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0 # initialize counter of the number of success to improve on the best observation\n",
    "    success_tolerance: int = 5  # Note: The original paper uses 3, this is the number of successes in a row needed to expand the region\n",
    "    best_value: float = -float(\"inf\") # best value so far, initialized to be the infimum\n",
    "    restart_triggered: bool = False \n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.failure_tolerance = math.ceil(\n",
    "            max([4.0 / self.batch_size, float(self.dim) / self.batch_size]) # number of failures needed in a row to shrink the trust region\n",
    "        )\n",
    "\n",
    "\n",
    "def update_state(state, Y_next):\n",
    "    # count if a success, otherwise a failure\n",
    "    if max(Y_next) > state.best_value + 1e-3 * math.fabs(state.best_value):\n",
    "        state.success_counter += 1\n",
    "        state.failure_counter = 0\n",
    "    else:\n",
    "        state.success_counter = 0\n",
    "        state.failure_counter += 1\n",
    "    # check if we need to expand or shrink the trust region\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "    # set the best value if we got a new observation\n",
    "    state.best_value = max(state.best_value, max(Y_next).item())\n",
    "    if state.length < state.length_min:\n",
    "        state.restart_triggered = True\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model using SingleTaskGP\n",
    "def create_model(train_x, train_y):\n",
    "    train_y = train_y.view(-1, 1)  # Ensure train_y has the correct shape\n",
    "    model = SingleTaskGP(train_x, train_y)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    return model, mll\n",
    "\n",
    "import gpytorch.settings as settings\n",
    "\n",
    "def fit_gpytorch_model(mll):\n",
    "    mll.train()\n",
    "    optimizer = torch.optim.Adam(mll.parameters(), lr=0.05)\n",
    "    training_iter = 300\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = mll.model(mll.model.train_inputs[0])\n",
    "        with settings.use_toeplitz(False), settings.max_cholesky_size(2000), settings.cholesky_jitter(1e-4):\n",
    "            loss = -mll(output, mll.model.train_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if i % 50 == 49:\n",
    "        #    print(f'Iter {i+1}/{training_iter} - Loss: {loss.item():.3f}   lengthscale: {mll.model.covar_module.base_kernel.lengthscale.detach()}   noise: {mll.model.likelihood.noise.item():.3f}')\n",
    "            \n",
    "# we use the model given in the tutorial, we also add the hyper-parameter training as a method\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # set a constant mean\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # use a simple RBF kernel with constant scaling\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=train_x.shape[1]))\n",
    "        # set number of hyper-parameter training iterations\n",
    "        self.training_iter = 300\n",
    "        self.num_outputs = 1  # Add this line\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    batch_size=1,  # fix batch size to 1\n",
    "    n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "    num_restarts=10,\n",
    "    raw_samples=512,\n",
    "    acqf=\"ts\",  # \"ei\", \"ucb\", or \"ts\"\n",
    "):\n",
    "    assert acqf in (\"ts\", \"ei\", \"ucb\",\"ucb0\")\n",
    "    assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "    if n_candidates is None:\n",
    "        n_candidates = min(10000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "    # Scale the trust region to be proportional to the lengthscales\n",
    "    x_center = X[Y.argmax(), :].clone()\n",
    "    weights = model.covar_module.base_kernel.lengthscale.squeeze().detach()\n",
    "    weights = weights / weights.mean()\n",
    "    weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "    tr_lb = torch.clamp(x_center - weights * state.length / 2.0, 0.0, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + weights * state.length / 2.0, 0.0, 1.0)\n",
    "    \n",
    "    dim = X.shape[-1]\n",
    "    sobol = SobolEngine(dim, scramble=True)\n",
    "    pert = sobol.draw(n_candidates)\n",
    "    pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "    # Create a perturbation mask\n",
    "    prob_perturb = min(20.0 / dim, 1.0)\n",
    "    mask = (\n",
    "        torch.rand(n_candidates, dim)\n",
    "        <= prob_perturb\n",
    "    )\n",
    "    ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "    mask[ind, torch.randint(0, dim - 1, size=(len(ind),))] = 1\n",
    "\n",
    "    # Create candidate points from the perturbations and the mask        \n",
    "    X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "    X_cand[mask] = pert[mask]\n",
    "    X_cand.requires_grad_(True)  # Ensure requires_grad is True for optimization\n",
    "\n",
    "    model.eval()\n",
    "    if acqf == \"ts\":\n",
    "        # Sample on the candidate points using Thompson Sampling\n",
    "        posterior_distribution = model.posterior(X_cand)\n",
    "        posterior_sample = posterior_distribution.sample()\n",
    "        X_next_idx = torch.argmax(posterior_sample)\n",
    "        X_next = X_cand[X_next_idx]\n",
    "    else:\n",
    "        if acqf == \"ei\":\n",
    "            # Expected Improvement acquisition function\n",
    "            acq_function = ExpectedImprovement(model=model, best_f=Y.max())\n",
    "        elif acqf == \"ucb\":\n",
    "            # Upper Confidence Bound acquisition function\n",
    "            acq_function = UpperConfidenceBound(model=model, beta=1.96)\n",
    "        elif acqf == \"ucb0\":\n",
    "            # Upper Confidence Bound acquisition function\n",
    "            acq_function = UpperConfidenceBound(model=model, beta=0.1)\n",
    "            \n",
    "        # Optimize the acquisition function\n",
    "        bounds = torch.stack([torch.zeros(dim), torch.ones(dim)]).to(X_cand.device)\n",
    "        X_next, _ = optimize_acqf(\n",
    "            acq_function,\n",
    "            bounds=bounds,\n",
    "            q=batch_size,\n",
    "            num_restarts=num_restarts,\n",
    "            raw_samples=raw_samples,\n",
    "        )\n",
    "        X_next.requires_grad_(True)  # Ensure X_next has requires_grad=True\n",
    "\n",
    "    return X_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_query_via_TurBO(train_x, train_y, turbo_state, training_iter=300, verbose=False):\n",
    "    model, mll = create_model(train_x, train_y)\n",
    "    fit_gpytorch_model(mll)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if verbose and (i % 50 == 49):\n",
    "        #    print(f'Iter {i+1}/{training_iter} - Loss: {loss.item():.3f}   lengthscale: {model.covar_module.base_kernel.lengthscale.detach()}   noise: {model.likelihood.noise.item():.3f}')\n",
    "\n",
    "    return generate_batch(turbo_state, model=model, X=train_x, Y=train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(np.load('initial_data/function_8/initial_inputs.npy')).to(torch.float32)\n",
    "train_y = torch.from_numpy(np.load('initial_data/function_8/initial_outputs.npy')).to(torch.float32)\n",
    "\n",
    "state = TurboState(dim = 2, best_value = torch.max(train_y).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# New data to append\n",
    "new_inputs = [\n",
    "    [0.111111, 0.111111, 0.111111, 0.111111, 0.111111, 0.111111, 0.111111, 0.111111],\n",
    "    [0.5, 1.0e-06, 0.25, 0.75, 0.25, 0.25, 0.25, 0.75],\n",
    "    [0.15, 0.15, 0.35, 0.55, 0.15, 0.725, 0.55, 0.95],\n",
    "    [0.333334, 0.5, 0.5, 0.333334, 0.333334, 0.333334, 0.666666, 0.5],\n",
    "    [0.200001, 0.200001, 0.4, 0.4, 0.200001, 0.799999, 0.6, 0.799999],\n",
    "    [0.157895, 0.052632, 0.157895, 0.333333, 0.210527, 0.333333, 0.31579, 0.5],\n",
    "    [0.25, 0.052632, 0.210527, 0.5, 0.210527, 0.5, 0.210527, 0.75],\n",
    "    [0.052632, 0.052632, 0.157895, 0.511111, 0.263158, 0.499999, 0.526316, 0.749999],\n",
    "    [1.0e-06, 0.2, 0.25, 1.0e-06, 0.75, 0.5, 0.25, 1.0e-06],\n",
    "    [1.0e-06, 0.33334, 1.0e-06, 1.0e-06, 0.999999, 0.5, 1.0e-06, 0.833333],\n",
    "    [0.052632, 0.473684, 1.0e-06, 1.0e-06, 0.526316, 1.0e-06, 0.105264, 1.0e-06],\n",
    "    [0.166667, 0.333334, 0.166667, 0.166667, 0.666666, 0.5, 0.166667, 0.5]\n",
    "]\n",
    "\n",
    "new_outputs = [\n",
    "    9.56743,\n",
    "    9.033300299999,\n",
    "    9.170675,\n",
    "    8.751078688886,\n",
    "    9.1023007399954,\n",
    "    9.7185386826085,\n",
    "    9.6273548350665,\n",
    "    9.4939825701369,\n",
    "    9.8695508199969,\n",
    "    9.7677466955466,\n",
    "    9.476142128,\n",
    "    9.941077316\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert new data to tensors\n",
    "new_train_x = torch.tensor(new_inputs, dtype=torch.float32)\n",
    "new_train_y = torch.tensor(new_outputs, dtype=torch.float32)\n",
    "\n",
    "# Append the new data to the existing tensors\n",
    "train_x = torch.cat((train_x, new_train_x), dim=0)\n",
    "train_y = torch.cat((train_y, new_train_y), dim=0)\n",
    "\n",
    "# Ensure train_x is within [0, 1] range\n",
    "scaler_x = MinMaxScaler()\n",
    "train_x = torch.tensor(scaler_x.fit_transform(train_x.numpy()), dtype=torch.float32)\n",
    "\n",
    "# Check for non-finite values in train_y and handle them\n",
    "if not torch.all(torch.isfinite(train_y)):\n",
    "    print(\"Non-finite values detected in train_y\")\n",
    "    finite_mask = torch.isfinite(train_y)\n",
    "    train_x = train_x[finite_mask]\n",
    "    train_y = train_y[finite_mask]\n",
    "\n",
    "# Ensure train_y is properly scaled (Standardization)\n",
    "scaler_y = StandardScaler()\n",
    "train_y = torch.tensor(scaler_y.fit_transform(train_y.reshape(-1, 1)).flatten(), dtype=torch.float32)\n",
    "\n",
    "# Convert to double precision for BoTorch\n",
    "train_x = train_x.double()\n",
    "train_y = train_y.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurboState(dim=2, batch_size=1, length=0.3, length_min=0.01, length_max=1.0, failure_counter=0, failure_tolerance=4, success_counter=0, success_tolerance=5, best_value=tensor(3.3213), restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "state = TurboState(dim=2, best_value=torch.max(train_y).float())\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macponcho/anaconda3/lib/python3.11/site-packages/botorch/models/utils/assorted.py:202: InputDataWarning: Input data is not standardized (mean = tensor([1.3547e-09], dtype=torch.float64), std = tensor([1.0235], dtype=torch.float64)). Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model, mll = create_model(train_x, train_y)\n",
    "fit_gpytorch_model(mll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_query = next_query_via_TurBO(train_x=train_x, train_y=train_y, turbo_state=state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS: [0.400150-0.600044]\n",
      "EI: [0.353403-0.997757]\n",
      "UCB: [0.394553-0.480802]\n",
      "UCB-Z: [0.427731-0.489973]\n"
     ]
    }
   ],
   "source": [
    "# Next query with TS\n",
    "formatted_row_ts = '-'.join(format(x.item(), \".6f\") for x in next_query.view(-1))\n",
    "print(\"TS:\", f\"[{formatted_row_ts}]\")\n",
    "\n",
    "# Get the next query point using EI\n",
    "next_query_ei = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ei\")\n",
    "formatted_row_ei = '-'.join(format(x.item(), \".6f\") for x in next_query_ei.view(-1))\n",
    "print(\"EI:\", f\"[{formatted_row_ei}]\")\n",
    "\n",
    "# Get the next query point using UCB\n",
    "next_query_ucb = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ucb\")\n",
    "formatted_row_ucb = '-'.join(format(x.item(), \".6f\") for x in next_query_ucb.view(-1))\n",
    "print(\"UCB:\", f\"[{formatted_row_ucb}]\")\n",
    "\n",
    "# Get the next query point using UCB-Z\n",
    "next_query_ucb0 = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ucb0\")\n",
    "formatted_row_ucb0 = '-'.join(format(x.item(), \".6f\") for x in next_query_ucb0.view(-1))\n",
    "print(\"UCB-Z:\", f\"[{formatted_row_ucb0}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurboState(dim=2, batch_size=1, length=0.3, length_min=0.01, length_max=1.0, failure_counter=0, failure_tolerance=4, success_counter=0, success_tolerance=5, best_value=tensor(3.3213), restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New observation for the previous query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback=0.666666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([84.6496], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_next = next_query  # The new query point from the last run\n",
    "y_next = torch.tensor(scaler_y.transform([[feedback]]).flatten(), dtype=torch.float64)\n",
    "print(y_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurboState(dim=2, batch_size=1, length=0.2, length_min=0.01, length_max=1.0, failure_counter=0, failure_tolerance=4, success_counter=1, success_tolerance=5, best_value=84.64962489984764, restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "new_state = update_state(state, y_next)\n",
    "print(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macponcho/anaconda3/lib/python3.11/site-packages/botorch/models/utils/assorted.py:202: InputDataWarning: Input data is not standardized (mean = tensor([3.4393], dtype=torch.float64), std = tensor([16.9469], dtype=torch.float64)). Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m train_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((train_y, y_next\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Execute the next run to get the next query point\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m next_query \u001b[38;5;241m=\u001b[39m next_query_via_TurBO(train_x\u001b[38;5;241m=\u001b[39mtrain_x, train_y\u001b[38;5;241m=\u001b[39mtrain_y, turbo_state\u001b[38;5;241m=\u001b[39mstate)\n",
      "Cell \u001b[0;32mIn[82], line 3\u001b[0m, in \u001b[0;36mnext_query_via_TurBO\u001b[0;34m(train_x, train_y, turbo_state, training_iter, verbose)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext_query_via_TurBO\u001b[39m(train_x, train_y, turbo_state, training_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     model, mll \u001b[38;5;241m=\u001b[39m create_model(train_x, train_y)\n\u001b[0;32m----> 3\u001b[0m     fit_gpytorch_model(mll)\n\u001b[1;32m      5\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(training_iter):\n",
      "Cell \u001b[0;32mIn[80], line 20\u001b[0m, in \u001b[0;36mfit_gpytorch_model\u001b[0;34m(mll)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39muse_toeplitz(\u001b[38;5;28;01mFalse\u001b[39;00m), settings\u001b[38;5;241m.\u001b[39mmax_cholesky_size(\u001b[38;5;241m2000\u001b[39m), settings\u001b[38;5;241m.\u001b[39mcholesky_jitter(\u001b[38;5;241m1e-4\u001b[39m):\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, mll\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_targets)\n\u001b[0;32m---> 20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# Append the new data to the existing training data\n",
    "train_x = torch.cat((train_x, x_next.view(1, -1)), dim=0)\n",
    "train_y = torch.cat((train_y, y_next.view(1)), dim=0)\n",
    "\n",
    "# Execute the next run to get the next query point\n",
    "next_query = next_query_via_TurBO(train_x=train_x, train_y=train_y, turbo_state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next chose query using TS: tensor([0.3146, 0.3899], dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "Next chosen query using EI: tensor([[0.2788, 0.9771]], requires_grad=True)\n",
      "Next chosen query using UCB: tensor([[0.3946, 0.4808]], requires_grad=True)\n",
      "Next chosen query using UCB-Z: tensor([[0.4277, 0.4900]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Nest query with TS\n",
    "print(f'Next chose query using TS: {next_query}')\n",
    "\n",
    "# Get the next query point using EI\n",
    "next_query_ei = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ei\")\n",
    "print(f'Next chosen query using EI: {next_query_ei}')\n",
    "\n",
    "# Get the next query point using UCB\n",
    "next_query_ucb = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ucb\")\n",
    "print(f'Next chosen query using UCB: {next_query_ucb}')\n",
    "\n",
    "# Get the next query point using UCB\n",
    "next_query_ucb = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ucb0\")\n",
    "print(f'Next chosen query using UCB-Z: {next_query_ucb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feel TuRBO would help you with the high-dimensional problems in the Capstone, give it a go! There a few questions that may lead to better performance:\n",
    "\n",
    "1. Maybe you can constraint some of the GPs hyper-parameters for better behaviour?\n",
    "2. How are you planning to initialize the Turbo State for the first time?\n",
    "\n",
    "So take the code from this notebook and modify it to your liking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6df83ed6fabd41a8e562c5a64e44b5d97b19c29150cbf5eb47fd88445500a37c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
