{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAY Q DEPURAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch.quasirandom import SobolEngine\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from dataclasses import dataclass\n",
    "import gpytorch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.acquisition import UpperConfidenceBound, ExpectedImprovement\n",
    "from dataclasses import dataclass\n",
    "from linear_operator.utils.errors import NotPSDError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a dataclass for our state\n",
    "@dataclass\n",
    "class TurboState:\n",
    "    dim: int # dimension of the problem, aka input dimension\n",
    "    batch_size: int = 1 # we could do batch optimization, but the capstone only does one query at a time\n",
    "    length: float = 0.8 # the length of the current trust region\n",
    "    length_min: float = 0.01 # minimum length for the trust region\n",
    "    length_max: float = 1.0 # maximum length for the trust region\n",
    "    failure_counter: int = 0 # initialize counter of the number of failures to improve on the best observation\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0 # initialize counter of the number of success to improve on the best observation\n",
    "    success_tolerance: int = 5  # Note: The original paper uses 3, this is the number of successes in a row needed to expand the region\n",
    "    best_value: float = -float(\"inf\") # best value so far, initialized to be the infimum\n",
    "    restart_triggered: bool = False \n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.failure_tolerance = math.ceil(\n",
    "            max([4.0 / self.batch_size, float(self.dim) / self.batch_size]) # number of failures needed in a row to shrink the trust region\n",
    "        )\n",
    "\n",
    "\n",
    "def update_state(state, Y_next):\n",
    "    # count if a success, otherwise a failure\n",
    "    if max(Y_next) > state.best_value + 1e-3 * math.fabs(state.best_value):\n",
    "        state.success_counter += 1\n",
    "        state.failure_counter = 0\n",
    "    else:\n",
    "        state.success_counter = 0\n",
    "        state.failure_counter += 1\n",
    "    # check if we need to expand or shrink the trust region\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "    # set the best value if we got a new observation\n",
    "    state.best_value = max(state.best_value, max(Y_next).item())\n",
    "    if state.length < state.length_min:\n",
    "        state.restart_triggered = True\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model using SingleTaskGP\n",
    "def create_model(train_x, train_y):\n",
    "    train_y = train_y.view(-1, 1)  # Ensure train_y has the correct shape\n",
    "    model = SingleTaskGP(train_x, train_y)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    return model, mll\n",
    "\n",
    "import gpytorch.settings as settings\n",
    "\n",
    "def fit_gpytorch_model(mll):\n",
    "    mll.train()\n",
    "    optimizer = torch.optim.Adam(mll.parameters(), lr=0.1)\n",
    "    training_iter = 200\n",
    "\n",
    "    # Check and modify hyperparameters\n",
    "    mll.model.covar_module.base_kernel.lengthscale = torch.clamp(mll.model.covar_module.base_kernel.lengthscale, min=1e-4)\n",
    "    mll.model.likelihood.noise = torch.clamp(mll.model.likelihood.noise, min=1e-4)\n",
    "    \n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = mll.model(mll.model.train_inputs[0])\n",
    "        with settings.use_toeplitz(False), settings.max_cholesky_size(2000), settings.cholesky_jitter(1e-1):\n",
    "            try:\n",
    "                loss = -mll(output, mll.model.train_targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            except NotPSDError as e:\n",
    "                print(f\"NotPSDError encountered at iteration {i}: {e}\")\n",
    "                break\n",
    "\n",
    "        #if i % 50 == 49:\n",
    "        #    print(f'Iter {i+1}/{training_iter} - Loss: {loss.item():.3f}   lengthscale: {mll.model.covar_module.base_kernel.lengthscale.detach()}   noise: {mll.model.likelihood.noise.item():.3f}')\n",
    "        \n",
    "    \n",
    "# we use the model given in the tutorial, we also add the hyper-parameter training as a method\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # set a constant mean\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # use a simple RBF kernel with constant scaling\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=train_x.shape[1]))\n",
    "        # set number of hyper-parameter training iterations\n",
    "        self.training_iter = 200\n",
    "        self.num_outputs = 1  # Add this line\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        covar_x += torch.eye(covar_x.size(-1)) * 1e-4  # Add jitter\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    batch_size=1,  # fix batch size to 1\n",
    "    n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "    num_restarts=10,\n",
    "    raw_samples=512,\n",
    "    acqf=\"ts\",  # \"ei\", \"ucb\", or \"ts\"\n",
    "):\n",
    "    assert acqf in (\"ts\", \"ei\", \"ucb\", \"ucb0\")\n",
    "    assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "    if n_candidates is None:\n",
    "        n_candidates = min(10000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "    # Scale the trust region to be proportional to the lengthscales\n",
    "    x_center = X[Y.argmax(), :].clone()\n",
    "    weights = model.covar_module.base_kernel.lengthscale.squeeze().detach()\n",
    "    weights = weights / weights.mean()\n",
    "    weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "    tr_lb = torch.clamp(x_center - weights * state.length / 2.0, 0.0, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + weights * state.length / 2.0, 0.0, 1.0)\n",
    "    \n",
    "    dim = X.shape[-1]\n",
    "    sobol = SobolEngine(dim, scramble=True)\n",
    "    pert = sobol.draw(n_candidates)\n",
    "    pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "    # Create a perturbation mask\n",
    "    prob_perturb = min(20.0 / dim, 1.0)\n",
    "    mask = (\n",
    "        torch.rand(n_candidates, dim)\n",
    "        <= prob_perturb\n",
    "    )\n",
    "    ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "    mask[ind, torch.randint(0, dim - 1, size=(len(ind),))] = 1\n",
    "\n",
    "    # Create candidate points from the perturbations and the mask        \n",
    "    X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "    X_cand[mask] = pert[mask]\n",
    "    X_cand.requires_grad_(True)  # Ensure requires_grad is True for optimization\n",
    "\n",
    "    model.eval()\n",
    "    if acqf == \"ts\":\n",
    "        # Sample on the candidate points using Thompson Sampling\n",
    "        with torch.no_grad():\n",
    "            posterior_distribution = model.posterior(X_cand)\n",
    "            posterior_sample = posterior_distribution.sample()\n",
    "            X_next_idx = torch.argmax(posterior_sample)\n",
    "            X_next = X_cand[X_next_idx]\n",
    "    else:\n",
    "        if acqf == \"ei\":\n",
    "            # Expected Improvement acquisition function\n",
    "            acq_function = ExpectedImprovement(model=model, best_f=Y.max())\n",
    "        elif acqf == \"ucb\":\n",
    "            # Upper Confidence Bound acquisition function\n",
    "            acq_function = UpperConfidenceBound(model=model, beta=1.96)\n",
    "        elif acqf == \"ucb0\":\n",
    "            # Upper Confidence Bound acquisition function\n",
    "            acq_function = UpperConfidenceBound(model=model, beta=0.1)\n",
    "            \n",
    "        # Optimize the acquisition function\n",
    "        bounds = torch.stack([torch.zeros(dim), torch.ones(dim)]).to(X_cand.device)\n",
    "        X_next, _ = optimize_acqf(\n",
    "            acq_function,\n",
    "            bounds=bounds,\n",
    "            q=batch_size,\n",
    "            num_restarts=num_restarts,\n",
    "            raw_samples=raw_samples,\n",
    "        )\n",
    "        X_next.requires_grad_(True)  # Ensure X_next has requires_grad=True\n",
    "\n",
    "    return X_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_query_via_TurBO(train_x, train_y, turbo_state, training_iter=200, verbose=False):\n",
    "    model, mll = create_model(train_x, train_y)\n",
    "    fit_gpytorch_model(mll)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #if verbose and (i % 50 == 49):\n",
    "        #    print(f'Iter {i+1}/{training_iter} - Loss: {loss.item():.3f}   lengthscale: {model.covar_module.base_kernel.lengthscale.detach()}   noise: {model.likelihood.noise.item():.3f}')\n",
    "\n",
    "    return generate_batch(turbo_state, model=model, X=train_x, Y=train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(np.load('initial_data/function_5/initial_inputs.npy')).to(torch.float32)\n",
    "train_y = torch.from_numpy(np.load('initial_data/function_5/initial_outputs.npy')).to(torch.float32)\n",
    "\n",
    "state = TurboState(dim = 4, best_value = torch.max(train_y).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data to append\n",
    "new_inputs = [\n",
    "    [0.111111, 0.111111, 0.111111, 0.111111], \n",
    "    [0.649999, 0.999999, 0.999999, 0.999999],\n",
    "    [0.3, 0.855263, 0.95, 0.9],\n",
    "    [0.999999, 0.999999, 0.999999, 0.999999],\n",
    "    [1.0e-06, 1.0e-06, 1.0e-06, 0.999999],\n",
    "    [0.999999, 0.85, 0.85, 0.85],\n",
    "    [0.947368, 0.999999, 0.999999, 0.999999],\n",
    "    [0.986206, 0.996551, 0.996551, 0.996551],\n",
    "    [0.900001, 0.900001, 0.999999, 0.900001],\n",
    "    [0.97, 0.999999, 0.97, 0.97],\n",
    "    [0.999999, 0.896551, 0.999999, 0.999999],\n",
    "    [0.793103, 0.586207, 0.689655, 0.482759]\n",
    "]\n",
    "\n",
    "new_outputs = [\n",
    "    161.986812,\n",
    "    5115.20243260104,\n",
    "    1677.31031094248,\n",
    "    8662.40500124829,\n",
    "    190.839622015135,\n",
    "    3096.47920847264,\n",
    "    7744.993860824856,\n",
    "    8207.32979758336,\n",
    "    4379.0465663322,\n",
    "    7074.98868236043,\n",
    "    7029.471643,\n",
    "    81.60809216\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert new data to tensors\n",
    "new_train_x = torch.tensor(new_inputs, dtype=torch.float32)\n",
    "new_train_y = torch.tensor(new_outputs, dtype=torch.float32)\n",
    "\n",
    "# Append the new data to the existing tensors\n",
    "train_x = torch.cat((train_x, new_train_x), dim=0)\n",
    "train_y = torch.cat((train_y, new_train_y), dim=0)\n",
    "\n",
    "#print(train_x)\n",
    "#print(train_y)\n",
    "\n",
    "# Ensure train_x is within [0, 1] range\n",
    "scaler_x = MinMaxScaler()\n",
    "train_x = torch.tensor(scaler_x.fit_transform(train_x.numpy()), dtype=torch.float32)\n",
    "\n",
    "# Check for non-finite values in train_y and handle them\n",
    "if not torch.all(torch.isfinite(train_y)):\n",
    "    print(\"Non-finite values detected in train_y\")\n",
    "    finite_mask = torch.isfinite(train_y)\n",
    "    train_x = train_x[finite_mask]\n",
    "    train_y = train_y[finite_mask]\n",
    "\n",
    "# Ensure train_y is properly scaled (Standardization)\n",
    "scaler_y = StandardScaler()\n",
    "train_y = torch.tensor(scaler_y.fit_transform(train_y.reshape(-1, 1)).flatten(), dtype=torch.float32)\n",
    "\n",
    "# Convert to double precision for BoTorch\n",
    "train_x = train_x.double()\n",
    "train_y = train_y.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurboState(dim=8, batch_size=1, length=0.8, length_min=0.01, length_max=1.0, failure_counter=0, failure_tolerance=8, success_counter=0, success_tolerance=5, best_value=tensor(1.5950), restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "state = TurboState(dim=4, best_value=torch.max(train_y).float())\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NotPSDError encountered at iteration 20: Matrix not positive definite after repeatedly adding jitter up to 1.0e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macponcho/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-08 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/macponcho/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-07 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/macponcho/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/macponcho/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/macponcho/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/macponcho/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-03 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model, mll = create_model(train_x, train_y)\n",
    "fit_gpytorch_model(mll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NotPSDError encountered at iteration 20: Matrix not positive definite after repeatedly adding jitter up to 1.0e-03.\n"
     ]
    },
    {
     "ename": "NotPSDError",
     "evalue": "Matrix not positive definite after repeatedly adding jitter up to 1.0e-03.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotPSDError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[241], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m next_query \u001b[38;5;241m=\u001b[39m next_query_via_TurBO(train_x\u001b[38;5;241m=\u001b[39mtrain_x, train_y\u001b[38;5;241m=\u001b[39mtrain_y, turbo_state\u001b[38;5;241m=\u001b[39mstate)\n",
      "Cell \u001b[0;32mIn[234], line 10\u001b[0m, in \u001b[0;36mnext_query_via_TurBO\u001b[0;34m(train_x, train_y, turbo_state, training_iter, verbose)\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_x)\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, train_y)\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/mlls/exact_marginal_log_likelihood.py:64\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[0;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m     63\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(function_dist, \u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 64\u001b[0m res \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlog_prob(target)\n\u001b[1;32m     65\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/distributions/multivariate_normal.py:171\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mSee :py:meth:`torch.distributions.Distribution.log_prob\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m<torch.distributions.distribution.Distribution.log_prob>`.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mfast_computations\u001b[38;5;241m.\u001b[39mlog_prob\u001b[38;5;241m.\u001b[39moff():\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlog_prob(value)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/distributions/multivariate_normal.py:248\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[1;32m    247\u001b[0m diff \u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\n\u001b[0;32m--> 248\u001b[0m M \u001b[38;5;241m=\u001b[39m _batch_mahalanobis(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril, diff)\n\u001b[1;32m    249\u001b[0m half_log_det \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril\u001b[38;5;241m.\u001b[39mdiagonal(dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, dim2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlog()\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    251\u001b[0m )\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m+\u001b[39m M) \u001b[38;5;241m-\u001b[39m half_log_det\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gpytorch/distributions/multivariate_normal.py:88\u001b[0m, in \u001b[0;36mMultivariateNormal._unbroadcasted_scale_tril\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unbroadcasted_scale_tril\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mislazy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__unbroadcasted_scale_tril \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;66;03m# cache root decoposition\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m         ust \u001b[38;5;241m=\u001b[39m to_dense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_covariance_matrix\u001b[38;5;241m.\u001b[39mcholesky())\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m ust\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__unbroadcasted_scale_tril\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:1303\u001b[0m, in \u001b[0;36mLinearOperator.cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;129m@_implements\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky)\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m], upper: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# returns TriangularLinearOperator\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LinearOperator.\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m    :param upper: Upper triangular or lower triangular factor (default: False).\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    :return: Cholesky factor (lower or upper triangular)\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cholesky(upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m   1305\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/operators/_linear_operator.py:522\u001b[0m, in \u001b[0;36mLinearOperator._cholesky\u001b[0;34m(self, upper)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(evaluated_mat\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt())\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# contiguous call is necessary here\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m cholesky \u001b[38;5;241m=\u001b[39m psd_safe_cholesky(evaluated_mat, upper\u001b[38;5;241m=\u001b[39mupper)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TriangularLinearOperator(cholesky, upper\u001b[38;5;241m=\u001b[39mupper)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:65\u001b[0m, in \u001b[0;36mpsd_safe_cholesky\u001b[0;34m(A, upper, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpsd_safe_cholesky\u001b[39m(A, upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, jitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_tries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m        :attr:`A` (Tensor):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m            Number of attempts (with successively increasing jitter) to make before raising an error.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     L \u001b[38;5;241m=\u001b[39m _psd_safe_cholesky(A, out\u001b[38;5;241m=\u001b[39mout, jitter\u001b[38;5;241m=\u001b[39mjitter, max_tries\u001b[38;5;241m=\u001b[39mmax_tries)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/linear_operator/utils/cholesky.py:47\u001b[0m, in \u001b[0;36m_psd_safe_cholesky\u001b[0;34m(A, out, jitter, max_tries)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(info):\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m L\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m NotPSDError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatrix not positive definite after repeatedly adding jitter up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjitter_new\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotPSDError\u001b[0m: Matrix not positive definite after repeatedly adding jitter up to 1.0e-03."
     ]
    }
   ],
   "source": [
    "next_query = next_query_via_TurBO(train_x=train_x, train_y=train_y, turbo_state=state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS: [0.400150-0.600044]\n",
      "EI: [0.353403-0.997757]\n",
      "UCB: [0.394553-0.480802]\n",
      "UCB-Z: [0.427731-0.489973]\n"
     ]
    }
   ],
   "source": [
    "# Next query with TS\n",
    "formatted_row_ts = '-'.join(format(x.item(), \".6f\") for x in next_query.view(-1))\n",
    "print(\"TS:\", f\"[{formatted_row_ts}]\")\n",
    "\n",
    "# Get the next query point using EI\n",
    "next_query_ei = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ei\")\n",
    "formatted_row_ei = '-'.join(format(x.item(), \".6f\") for x in next_query_ei.view(-1))\n",
    "print(\"EI:\", f\"[{formatted_row_ei}]\")\n",
    "\n",
    "# Get the next query point using UCB\n",
    "next_query_ucb = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ucb\")\n",
    "formatted_row_ucb = '-'.join(format(x.item(), \".6f\") for x in next_query_ucb.view(-1))\n",
    "print(\"UCB:\", f\"[{formatted_row_ucb}]\")\n",
    "\n",
    "# Get the next query point using UCB-Z\n",
    "next_query_ucb0 = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ucb0\")\n",
    "formatted_row_ucb0 = '-'.join(format(x.item(), \".6f\") for x in next_query_ucb0.view(-1))\n",
    "print(\"UCB-Z:\", f\"[{formatted_row_ucb0}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurboState(dim=2, batch_size=1, length=0.3, length_min=0.01, length_max=1.0, failure_counter=0, failure_tolerance=4, success_counter=0, success_tolerance=5, best_value=tensor(3.3213), restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New observation for the previous query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback=0.666666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([84.6496], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_next = next_query  # The new query point from the last run\n",
    "y_next = torch.tensor(scaler_y.transform([[feedback]]).flatten(), dtype=torch.float64)\n",
    "print(y_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurboState(dim=2, batch_size=1, length=0.2, length_min=0.01, length_max=1.0, failure_counter=0, failure_tolerance=4, success_counter=1, success_tolerance=5, best_value=84.64962489984764, restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "new_state = update_state(state, y_next)\n",
    "print(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macponcho/anaconda3/lib/python3.11/site-packages/botorch/models/utils/assorted.py:202: InputDataWarning: Input data is not standardized (mean = tensor([3.4393], dtype=torch.float64), std = tensor([16.9469], dtype=torch.float64)). Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m train_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((train_y, y_next\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Execute the next run to get the next query point\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m next_query \u001b[38;5;241m=\u001b[39m next_query_via_TurBO(train_x\u001b[38;5;241m=\u001b[39mtrain_x, train_y\u001b[38;5;241m=\u001b[39mtrain_y, turbo_state\u001b[38;5;241m=\u001b[39mstate)\n",
      "Cell \u001b[0;32mIn[82], line 3\u001b[0m, in \u001b[0;36mnext_query_via_TurBO\u001b[0;34m(train_x, train_y, turbo_state, training_iter, verbose)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnext_query_via_TurBO\u001b[39m(train_x, train_y, turbo_state, training_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     model, mll \u001b[38;5;241m=\u001b[39m create_model(train_x, train_y)\n\u001b[0;32m----> 3\u001b[0m     fit_gpytorch_model(mll)\n\u001b[1;32m      5\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(training_iter):\n",
      "Cell \u001b[0;32mIn[80], line 20\u001b[0m, in \u001b[0;36mfit_gpytorch_model\u001b[0;34m(mll)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39muse_toeplitz(\u001b[38;5;28;01mFalse\u001b[39;00m), settings\u001b[38;5;241m.\u001b[39mmax_cholesky_size(\u001b[38;5;241m2000\u001b[39m), settings\u001b[38;5;241m.\u001b[39mcholesky_jitter(\u001b[38;5;241m1e-4\u001b[39m):\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, mll\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain_targets)\n\u001b[0;32m---> 20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# Append the new data to the existing training data\n",
    "train_x = torch.cat((train_x, x_next.view(1, -1)), dim=0)\n",
    "train_y = torch.cat((train_y, y_next.view(1)), dim=0)\n",
    "\n",
    "# Execute the next run to get the next query point\n",
    "next_query = next_query_via_TurBO(train_x=train_x, train_y=train_y, turbo_state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next chose query using TS: tensor([0.3146, 0.3899], dtype=torch.float64, grad_fn=<SelectBackward0>)\n",
      "Next chosen query using EI: tensor([[0.2788, 0.9771]], requires_grad=True)\n",
      "Next chosen query using UCB: tensor([[0.3946, 0.4808]], requires_grad=True)\n",
      "Next chosen query using UCB-Z: tensor([[0.4277, 0.4900]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Nest query with TS\n",
    "print(f'Next chose query using TS: {next_query}')\n",
    "\n",
    "# Get the next query point using EI\n",
    "next_query_ei = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ei\")\n",
    "print(f'Next chosen query using EI: {next_query_ei}')\n",
    "\n",
    "# Get the next query point using UCB\n",
    "next_query_ucb = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ucb\")\n",
    "print(f'Next chosen query using UCB: {next_query_ucb}')\n",
    "\n",
    "# Get the next query point using UCB\n",
    "next_query_ucb = generate_batch(state, model=model, X=train_x, Y=train_y, acqf=\"ucb0\")\n",
    "print(f'Next chosen query using UCB-Z: {next_query_ucb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feel TuRBO would help you with the high-dimensional problems in the Capstone, give it a go! There a few questions that may lead to better performance:\n",
    "\n",
    "1. Maybe you can constraint some of the GPs hyper-parameters for better behaviour?\n",
    "2. How are you planning to initialize the Turbo State for the first time?\n",
    "\n",
    "So take the code from this notebook and modify it to your liking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6df83ed6fabd41a8e562c5a64e44b5d97b19c29150cbf5eb47fd88445500a37c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
